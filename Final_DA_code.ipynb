{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23xwkoMmevel"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVKRXtBsgATC"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2Rvtb_lMFXZ"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "adzbyfabexpv"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from pyspark.sql import SparkSession\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml import PipelineModel\n",
        "from pydantic import BaseModel\n",
        "from typing import Union\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import pandas as ps\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "data_path = '/content/drive/MyDrive/VNU_IS/HK VI/Advanced DA/Final/House_sale_final.csv'\n",
        "mydata = spark.read.csv(data_path, header = True, inferSchema=True)\n",
        "#Choose 4-5 models to build ML\n",
        "models = {1: \"model_lr_weight\",       #Linear regression \n",
        "          2: \"model_rd_weight\",       #Random Forest\n",
        "          3: \"model_gbt_weight\",      #GBTRegressor\n",
        "          4: \"model_dt_weight\",       #Decision Tree\n",
        "          5: \"model_gn_weight\"}      #GeneralizedLinearRegression\n",
        "\n",
        "class Item(BaseModel):\n",
        "    price: Union[float, None]\n",
        "    bedrooms: int\n",
        "    bathrooms: int\n",
        "    sqft_living: int\n",
        "    sqft_lot: int\n",
        "    floors: int\n",
        "    waterfront: int\n",
        "    view: int\n",
        "    condition: int\n",
        "    sqft_above: int\n",
        "    sqft_basement: int\n",
        "    yr_built: int\n",
        "    yr_renovated: int\n",
        "    city: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Hello class to our FastAPI Final project\"}\n",
        "\n",
        "# https://34ac-34-74-111-174.ngrok.io/getinfo\n",
        "#Display dataset info\n",
        "@app.get(\"/getinfo\")\n",
        "async def getinfo():\n",
        "    columns = mydata.columns\n",
        "    num_rows = mydata.count()\n",
        "\n",
        "    jsons = {'columns': columns,\n",
        "             'num_rows': num_rows,\n",
        "             }\n",
        "    return jsons\n",
        "\n",
        "# https://34ac-34-74-111-174.ngrok.io/data/?row_id=5&col_id=4\n",
        "#Display data by row and column id\n",
        "@app.get(\"/data/\")\n",
        "async def get_data_by_row_and_col_id(row_id: int, col_id: int):\n",
        "    column_name = mydata.columns[col_id]\n",
        "    column_value = mydata.select(column_name).collect()[row_id][0]\n",
        "\n",
        "    return {'row_id': row_id,\n",
        "            'col_id': col_id,\n",
        "            'column_name': column_name,\n",
        "            'column_value': column_value}\n",
        "\n",
        "# https://2bb4-34-74-111-174.ngrok.io/data1/?row_id=5\n",
        "#Display data by row id\n",
        "@app.get(\"/data1/\")\n",
        "async def get_data_by_row_id(row_id: int):\n",
        "    row = mydata.take(row_id + 1)[-1]\n",
        "    row_dict = row.asDict()\n",
        "    return row_dict\n",
        "\n",
        "# https://dc50-34-74-111-174.ngrok.io/train/?item_id=5\n",
        "@app.get(\"/train/\")                 \n",
        "async def train(item_id: int):\n",
        "    #Label encoder\n",
        "    from pyspark.sql.functions import col\n",
        "    from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "    # Create a StringIndexer object and encode \"city\" column\n",
        "    stringIndexer = StringIndexer(inputCol= \"city\", outputCol=\"city_encoder\")\n",
        "    model_encode = stringIndexer.fit(mydata)\n",
        "    encode_data = model_encode.transform(mydata)\n",
        "    #encode_data.select(\"city\", \"city_encoder\").show()\n",
        "\n",
        "    #City encoder output\n",
        "    before_transform = encode_data.select('city').distinct()\n",
        "    after_transform = encode_data.select('city_encoder').distinct()\n",
        "    city_encode = encode_data.select(\"city\", \"city_encoder\").distinct().toPandas()\n",
        "    #city_encode.head(20)\n",
        "    #Drop city column\n",
        "    final_data = encode_data.drop(\"city\")\n",
        "    #final_data.show(10)  \n",
        "\n",
        "    # Create a vector assembler to combine all the feature columns into a single vector column\n",
        "    assembler = VectorAssembler(inputCols=[\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"condition\", \n",
        "                                           \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"city_encoder\"], outputCol=\"features\")\n",
        "\n",
        "    # Apply the vector assembler to the dataset\n",
        "    #final_data = assembler.transform(final_data)\n",
        "    encode_data = assembler.transform(encode_data)\n",
        "    # Split the dataset into training and test sets\n",
        "    #(trainingData, testData) = final_data.randomSplit([0.7, 0.3], seed = 2023)\n",
        "    (trainingData, testData) = encode_data.randomSplit([0.7, 0.3], seed = 2023)\n",
        "    #print(\"Training Dataset Count: \" + str(training_data.count()))\n",
        "    #print(\"Test Dataset Count: \" + str(test_data.count()))\n",
        "      \n",
        "    if item_id > max(models.keys()):    #Build models\n",
        "        return {\"Error\"}\n",
        "    elif item_id == 1:\n",
        "        # Create a Linear Regression model\n",
        "        from pyspark.ml.regression import LinearRegression\n",
        "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
        "        #Saving the Pipeline (weight model)\n",
        "        lr_pipeline  = Pipeline(stages = [lr])\n",
        "   \n",
        "        model = lr_pipeline.fit(trainingData)\n",
        "        \n",
        "        model.write().overwrite().save('model_lr_weight')         \n",
        "    elif item_id == 2:\n",
        "        #Create a Random Forest Regressor\n",
        "        from pyspark.ml.regression import RandomForestRegressor\n",
        "        rd = RandomForestRegressor(featuresCol=\"features\", labelCol=\"price\", maxBins = 32)\n",
        "        #Saving the Pipeline (weight model)\n",
        "        rd_pipeline  = Pipeline(stages = [rd])\n",
        "   \n",
        "        model = rd_pipeline.fit(trainingData)\n",
        "        \n",
        "        model.write().overwrite().save('model_rd_weight')    \n",
        "    elif item_id == 3:\n",
        "        # Create GB Boost Regression model\n",
        "        from pyspark.ml.regression import GBTRegressor\n",
        "        gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"price\")\n",
        "\n",
        "        rd_pipeline  = Pipeline(stages = [gbt])\n",
        "   \n",
        "        model = rd_pipeline.fit(trainingData)\n",
        "        \n",
        "        model.write().overwrite().save('model_gbt_weight')  \n",
        "\n",
        "    elif item_id == 4:\n",
        "        # Create Decision Tree Regression model\n",
        "        from pyspark.ml.regression import DecisionTreeRegressor\n",
        "        dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"price\")\n",
        "        \n",
        "        dt_pipeline  = Pipeline(stages = [dt])\n",
        "   \n",
        "        model = dt_pipeline.fit(trainingData)\n",
        "        \n",
        "        model.write().overwrite().save('model_dt_weight')  \n",
        "    elif item_id == 5:\n",
        "        # Create Generalized Linear Regression model\n",
        "        from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "        gn = GeneralizedLinearRegression(featuresCol=\"features\", labelCol=\"price\", maxIter = 10)\n",
        "        gn_pipeline  = Pipeline(stages = [gn])\n",
        "   \n",
        "        model = gn_pipeline.fit(trainingData)\n",
        "        \n",
        "        model.write().overwrite().save('model_gn_weight')  \n",
        "    # Evaluate model performance on the test set\n",
        "    predictions = model.transform(testData)\n",
        "    evaluator1 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
        "    rmse = evaluator1.evaluate(predictions)\n",
        "\n",
        "    evaluator2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"mae\")\n",
        "    mae = evaluator2.evaluate(predictions)\n",
        "\n",
        "    evaluator3 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"r2\")\n",
        "    r2 = evaluator3.evaluate(predictions)\n",
        "\n",
        "    # Print the root mean squared error (RMSE)\n",
        "    return {\"R-squared (R2):\": r2,\n",
        "            'MAE:': mae,\n",
        "            'RMSE:': rmse,\n",
        "            \"city_encode:\" : city_encode}\n",
        "\n",
        "# https://ec11-34-74-111-174.ngrok.io/predictions/?id=5&bedrooms=3&bathrooms=2&sqft_living=1000&sqft_lot=1500&floors=2&waterfront=2&view=3&condition=4&sqft_above=1250&sqft_basement=250&yr_built=2000&yr_renovated=2015&city_encoder=25            \n",
        "#Predictions\n",
        "@app.get(\"/predictions/\")\n",
        "async def get_predictions(id:int, bedrooms: int, bathrooms: int, sqft_living: int, sqft_lot: int, \n",
        "                          floors: int, waterfront: int, view: int, condition: int, sqft_above: int,\n",
        "                          sqft_basement: int, yr_built: int, yr_renovated: int, city_encoder: int):\n",
        "\n",
        "    x = [bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition,\n",
        "                    sqft_above, sqft_basement, yr_built, yr_renovated, city_encoder]\n",
        "  \n",
        "    d_f = ps.DataFrame(columns=[\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"condition\",\n",
        "                                \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"city_encoder\"])\n",
        "  \n",
        "    d_f.loc[len(d_f)] = x \n",
        "  \n",
        "    d_f = spark.createDataFrame(d_f)\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=[\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \n",
        "                                           \"condition\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\", \"city_encoder\"], outputCol=\"features\")\n",
        "    \n",
        "    output = assembler.transform(d_f)\n",
        "\n",
        "    pipelineModel = PipelineModel.load(models[id])\n",
        "  \n",
        "    y = pipelineModel.transform(output)\n",
        "\n",
        "    return{'prediction: ' : y.collect()[0][-1]}\n",
        "\n",
        "#Add new value into dataset\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "# Define the schema for the dataframe\n",
        "schema = StructType([\n",
        "    StructField(\"price\", IntegerType(), True),\n",
        "    StructField(\"bedrooms\", IntegerType(), True),\n",
        "    StructField(\"bathrooms\", IntegerType(), True),\n",
        "    StructField(\"sqft_living\", IntegerType(), True),\n",
        "    StructField(\"sqft_lot\", IntegerType(), True),\n",
        "    StructField(\"floors\", IntegerType(), True),\n",
        "    StructField(\"waterfront\", IntegerType(), True),\n",
        "    StructField(\"view\", IntegerType(), True),\n",
        "    StructField(\"condition\", IntegerType(), True),\n",
        "    StructField(\"sqft_above\", IntegerType(), True),\n",
        "    StructField(\"sqft_basement\", IntegerType(), True),\n",
        "    StructField(\"yr_built\", IntegerType(), True),\n",
        "    StructField(\"yr_renovated\", IntegerType(), True),\n",
        "    StructField(\"city\", StringType(), True)\n",
        "])\n",
        "\n",
        "# https://ec11-34-74-111-174.ngrok.io/add_house_data/?price=2000000&bedrooms=2&bathrooms=1&sqft_living=750&sqft_lot=1000&floors=1&waterfront=3&view=3&condition=3&sqft_above=750&sqft_basement=250&yr_built=1999&yr_renovated=2008&city=HaNoi\n",
        "@app.get(\"/add_house_data/\")       \n",
        "async def add_house_data(price: int, bedrooms: int, bathrooms: int, sqft_living: int, sqft_lot: int, \n",
        "                         floors: int, waterfront: int, view: int, condition: int, \n",
        "                         sqft_above: int, sqft_basement: int, yr_built: int, yr_renovated: int, city: str):\n",
        "    # Create a new row of data as a tuple\n",
        "     new_row = (price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition,\n",
        "               sqft_above, sqft_basement, yr_built, yr_renovated, city)\n",
        "    # Create a new dataframe with the new row\n",
        "    new_df = spark.createDataFrame([new_row], schema=schema)\n",
        "    # Append the new dataframe to the original dataframe\n",
        "    data_path = '/content/drive/MyDrive/VNU_IS/HK VI/Advanced DA/Final/House_sale_final.csv'\n",
        "    mydata = spark.read.csv(data_path, header = True, inferSchema=True)\n",
        "    mydata1 = mydata.union(new_df)\n",
        "    # Return the updated dataframe\n",
        "    return mydata1.toPandas().to_dict(orient='records')\n",
        "\n",
        "#Run FastAPI in Colab\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)\n",
        "\n",
        "#PROJECT DONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrgljR-Sf-Si"
      },
      "outputs": [],
      "source": [
        "#Test\n",
        "x = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "d_f = pd.DataFrame(columns=[\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"condition\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\"])\n",
        "d_f.loc[len(d_f)] = x \n",
        "d_f = spark.createDataFrame(d_f)\n",
        "assembler = VectorAssembler(inputCols=[\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"condition\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\"], outputCol=\"features\")\n",
        "\n",
        "output = assembler.transform(d_f)\n",
        "\n",
        "pipelineModel = PipelineModel.load(\"model_weight\")\n",
        "\n",
        "y = pipelineModel.transform(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO_b9xaAz2l8",
        "outputId": "63a69423-58db-41a9-adcf-7c2b338078d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4421435.433015432"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.collect()[0][-1]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
